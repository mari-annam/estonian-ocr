llama-2:
  num_train_epochs: 5
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: True
  optim: 'paged_adamw_32bit'
  logging_steps: 10
  save_strategy: 'epoch'
  learning_rate: 2e-4
  bf16: True
  fp16: False
  tf32: True
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  lr_scheduler_type: 'constant'
